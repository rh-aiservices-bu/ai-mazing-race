{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "598efbed-a559-485f-848f-3af77d90ff21",
   "metadata": {},
   "source": [
    "# Predicting Baklava Cravings: A Sweet Data Science Problem üç≠\n",
    "\n",
    "Did you know that Istanbul alone devours a staggering 2,000 tons of baklava during the festive season? That's a lot of sweet treats!\n",
    "\n",
    "To ensure that bakeries and retailers can meet this incredible demand without overproducing or understocking, we can leverage the power of data science. By analyzing historical sales data, consumer trends, and external factors like holidays, we can build a predictive model to forecast baklava demand with impressive accuracy.\n",
    "\n",
    "You are asked to experiment with different the hyperparameters, and store your experiments in the Model Registry to compare the results in order to deploy the best performing one!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56587e2f-0c9f-45a2-b63d-aa7dad3adf0a",
   "metadata": {},
   "source": [
    "# üê† Install & Import packages\n",
    "We will need to install and import packages as we develop our model.\n",
    "\n",
    "This will take a couple of minutes, and if pip gives any error, don't worry about it. Things will just run fine regardless."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e07fde4-aaa2-4bb3-b82b-5650e87195a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install keras \"tensorflow==2.15.1\" \"tf2onnx\" \"onnx\" \"seaborn\" \"onnxruntime\" model-registry==\"0.2.9\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba66d9b4-dab6-4e0e-b41a-eb7d90d76522",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pickle\n",
    "import os\n",
    "import logging, warnings\n",
    "\n",
    "# Suppress warnings\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "logging.getLogger('tensorflow').setLevel(logging.ERROR)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import tf2onnx\n",
    "import onnx\n",
    "import tensorflow as tf\n",
    "import joblib\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "import onnxruntime as ort\n",
    "\n",
    "\n",
    "# Suppress CUDA and TF warnings\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "logging.getLogger('tensorflow').setLevel(logging.ERROR)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11dd63e6-f901-40de-b598-dbb5d654372b",
   "metadata": {},
   "source": [
    "# üì¶ Load Data\n",
    "Let's load our dataset, that consist of baklava consumption information alongside the number of bakeries for the past 10 years in different regions of T√ºrkiye.\n",
    "\n",
    "Then we will select the input and output data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149c2557-b7d9-4f9f-8d29-0ff383a325fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "data = pd.read_csv(\"synthetic_baklava_data_turkey.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316bc4e1-84fd-43b3-8cf0-508ceeeb8041",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering\n",
    "data['Population_per_Bakery'] = data['Population'] / data['Bakery_Count'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8378a50-2180-4843-a3fd-1b8d6fd1b7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode the 'Region' column\n",
    "region_encoder = OneHotEncoder(sparse_output=False)\n",
    "region_encoded = region_encoder.fit_transform(data[[\"Region\"]])\n",
    "region_encoded_df = pd.DataFrame(region_encoded, columns=region_encoder.get_feature_names_out([\"Region\"]))\n",
    "data = pd.concat([data, region_encoded_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd34bfe-0ce3-4597-b031-90077733357f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of regions\n",
    "regions = list(data.columns[data.columns.str.startswith('Region_')])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611c2943",
   "metadata": {},
   "source": [
    "\n",
    "Input data (X) contains baklava consumption per region in the country in each day, with a detail whether it was a holiday season or not.\n",
    "\n",
    "Output data (y) is the target variable the model is trying to predict. In this case, y is the 'Demand' column which represents the demand for the upcoming holiday season. The model will learn to predict the demand based on the previous consumptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222d93fd-80d1-4dd9-9a2f-1d01280808a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for modeling\n",
    "features = ['Holiday_Promotion', 'Population_per_Bakery', 'Income_Level', 'Holiday_Season'] + regions\n",
    "X = data[features] \n",
    "y = data['Demand']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc4fed5-e304-43ac-80b6-1df1e7553027",
   "metadata": {},
   "source": [
    "# üöÄ Build the model\n",
    "\n",
    "This is where we need your help! Experimentation and exploration are key to finding the best settings for our specific dataset and problem. \n",
    "\n",
    "Below 4 little parameteres that are called as hyperparameters and we need your help to define the best settings for the model!\n",
    "\n",
    "It has some good starting values, but they can be better for sure!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9522683-7a97-45c2-a330-a527911b491c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epochs represents the number of times the model sees the entire training dataset. \n",
    "# Higher values can improve accuracy but may also lead to overfitting.\n",
    "epochs = 100  \n",
    "\n",
    "# batch_size is the number of samples processed before the model updates its internal parameters. \n",
    "# Larger batch sizes can speed up training but may require more memory.\n",
    "batch_size = 32  \n",
    "\n",
    "# learning_rate controls the step size during weight updates. \n",
    "# Higher values can lead to faster convergence but may cause instability if too high.\n",
    "learning_rate = 0.001  \n",
    "\n",
    "# Number of neurons in the hidden layer. For simpler problems, fewer neurons may suffice. \n",
    "# For more complex problems, a larger number of neurons may be necessary.\n",
    "# value can be 16, 32, 64..\n",
    "hidden_layer_units = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118120b6",
   "metadata": {},
   "source": [
    "The below piece of code is the model definition. It will uses the parameters you define up there and learned to predict the demand as accurately as possible.\n",
    "\n",
    "Then, we check how well the model is doing at making the guesses!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f16910-9f57-49a4-bd51-6ce98eac0eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_nn():\n",
    "    \"\"\"\n",
    "    Train and evaluate a simple neural network model.\n",
    "\n",
    "    Args:\n",
    "        X_train_scaled: Scaled training features.\n",
    "        y_train: Training target values.\n",
    "        X_test_scaled: Scaled test features.\n",
    "        y_test: Test target values.\n",
    "        hidden_layer_units: Number of units in hidden layer. Defaults to 64.\n",
    "        learning_rate: Learning rate for optimizer. Defaults to 0.001.\n",
    "        epochs: Number of training epochs. Defaults to 100.\n",
    "        batch_size: Batch size for training. Defaults to 32.\n",
    "\n",
    "    Returns:\n",
    "        Tuple containing:\n",
    "            - trained Keras Sequential model\n",
    "            - Mean Squared Error (MSE) on test set\n",
    "            - Mean Absolute Error (MAE) on test set\n",
    "    \"\"\"\n",
    "    # Define the model architecture\n",
    "    model = Sequential()\n",
    "    model.add(\n",
    "        Dense(\n",
    "            hidden_layer_units,\n",
    "            activation='relu',\n",
    "            input_shape=(X_train_scaled.shape[1],)\n",
    "        )\n",
    "    )\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    # Compile the model\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    model.compile(loss='mean_squared_error', optimizer=optimizer)\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        X_train_scaled,\n",
    "        y_train,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "    # Calculate metrics\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "    # Print training parameters and metrics\n",
    "    print(f\"Epochs: {epochs}\")\n",
    "    print(f\"Batch Size: {batch_size}\")\n",
    "    print(f\"Learning Rate: {learning_rate}\")\n",
    "    print(f\"Hidden Layer Units: {hidden_layer_units}\")\n",
    "    print(f\"Mean Squared Error: {mse:.4f}\")\n",
    "    print(f\"Mean Absolute Error: {mae:.4f}\")\n",
    "\n",
    "    # Plot training history\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.title('Model Loss During Training')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    return model, mse, mae"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3c5f60-a922-4840-876d-62c29fb15c72",
   "metadata": {},
   "source": [
    "# üèÉ Train & Evaluate the Model\n",
    "\n",
    "Let's kick of the training then! You'll get a nice plot and 2 important metrics to decide how well your parameters did.\n",
    "\n",
    "\n",
    "### Interpretation of the Loss Plot\n",
    "**X-axis:** Represents the number of training epochs.\n",
    "\n",
    "**Y-axis:** Represents the training loss, which measures how well the model is fitting the training data. A lower loss generally indicates better performance.\n",
    "\n",
    "**Decreasing Loss:** Ideally, the plot should show a downward trend, indicating that the model is learning and improving.\n",
    "\n",
    "**Plateau/Increase:** If the loss plateaus or starts to increase, it might suggest overfitting or an inappropriate learning rate.\n",
    "\n",
    "Experimentation and exploration are key to finding the best hyperparameter values for your specific dataset and problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd28215-47f3-4adc-ae3f-0106f10a9b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate the model\n",
    "model, mse, mae = train_and_evaluate_nn() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ead458",
   "metadata": {},
   "source": [
    "# ü´° Save the Model\n",
    "\n",
    "Here we convert our trained prediction model into a popular format called ONNX so we can serve it from OpenShift AI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112e9927",
   "metadata": {},
   "source": [
    "### üö®üö® please make sure you update the below cell üö®üö®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81f1acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üö®üö® Specify a version - use your name to make it unique üö®üö®\n",
    "# example\n",
    "# version = \"user1-0.0.1\"\n",
    "version = \"<YOUR_USER_NAME>-<VERSION>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01544b9f-60c1-4664-a88b-85e38666924e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create the artifacts directory if it doesn't exist\n",
    "artifact_path = f\"models/{version}/baklava/1/artifacts\"\n",
    "Path(artifact_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save the model to ONNX format\n",
    "onnx_model, _ = tf2onnx.convert.from_keras(\n",
    "    model, \n",
    "    input_signature=[tf.TensorSpec([None, X_train_scaled.shape[1]], tf.float32, name='input')]\n",
    ")\n",
    "onnx.save(onnx_model, f\"models/{version}/baklava/1/model.onnx\")\n",
    "\n",
    "# Save the scaler\n",
    "joblib.dump(scaler, f\"models/{version}/baklava/1/artifacts/scaler.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36dd30d8",
   "metadata": {},
   "source": [
    "# ü™£ Upload your model to the bucket for (possible) deployment\n",
    "Let's upload it to MinIO `models` bucket _in case this is the best performing one_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39dd7a30-d096-4cdb-9071-566cdbf7b976",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import botocore\n",
    "\n",
    "aws_access_key_id = os.environ.get('AWS_ACCESS_KEY_ID')\n",
    "aws_secret_access_key = os.environ.get('AWS_SECRET_ACCESS_KEY')\n",
    "endpoint_url = os.environ.get('AWS_S3_ENDPOINT')\n",
    "region_name = os.environ.get('AWS_DEFAULT_REGION')\n",
    "bucket_name = os.environ.get('AWS_S3_BUCKET')\n",
    "\n",
    "if not all([aws_access_key_id, aws_secret_access_key, endpoint_url, region_name, bucket_name]):\n",
    "    raise ValueError(\"One or data connection variables are empty.  \"\n",
    "                     \"Please check your data connection to an S3 bucket.\")\n",
    "\n",
    "session = boto3.session.Session(aws_access_key_id=aws_access_key_id,\n",
    "                                aws_secret_access_key=aws_secret_access_key)\n",
    "\n",
    "s3_resource = session.resource(\n",
    "    's3',\n",
    "    config=botocore.client.Config(signature_version='s3v4'),\n",
    "    endpoint_url=endpoint_url,\n",
    "    region_name=region_name)\n",
    "\n",
    "bucket = s3_resource.Bucket(bucket_name)\n",
    "\n",
    "\n",
    "def upload_directory_to_s3(local_directory, s3_prefix):\n",
    "    num_files = 0\n",
    "    for root, dirs, files in os.walk(local_directory):\n",
    "        for filename in files:\n",
    "            file_path = os.path.join(root, filename)\n",
    "            relative_path = os.path.relpath(file_path, local_directory)\n",
    "            s3_key = os.path.join(s3_prefix, relative_path)\n",
    "            print(f\"{file_path} -> {s3_key}\")\n",
    "            bucket.upload_file(file_path, s3_key)\n",
    "            num_files += 1\n",
    "    return num_files\n",
    "\n",
    "\n",
    "def list_objects(prefix):\n",
    "    filter = bucket.objects.filter(Prefix=prefix)\n",
    "    for obj in filter.all():\n",
    "        print(obj.key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2d2809-a895-4b10-8fea-0c2da87b81a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_models_directory = f\"models/{version}/baklava\"\n",
    "\n",
    "if not os.path.isdir(local_models_directory):\n",
    "    raise ValueError(f\"The directory '{local_models_directory}' does not exist.  \"\n",
    "                     \"Did you finish training the model in the previous notebook?\")\n",
    "\n",
    "num_files = upload_directory_to_s3(\"models\", \"models\")\n",
    "\n",
    "if num_files == 0:\n",
    "    raise ValueError(\"No files uploaded.  Did you finish training and \"\n",
    "                     \"saving the model to the \\\"models\\\" directory?  \"\n",
    "                     f\"Check for \\\"models/{version}/baklava/1/model.onnx\\\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8873c3",
   "metadata": {},
   "source": [
    "# üì´ Store Your Experiment and Deploy the BEST one\n",
    "\n",
    "Now, it's time to save this experiment! We need you to store below information in Model Registry for each of your experiment. Go back to OpenShift AI Dashboard. On the left hand side, you'll see a `Model Registry` created for you (and for everybody else joining this effort). Register your model by providing the below information: \n",
    "\n",
    "- Model Name and Version\n",
    "- Model Location (Path)\n",
    "- Number of Epochs\n",
    "- Batch Size\n",
    "- Learning Rate\n",
    "- Hidden Layer Units\n",
    "- Mean Squared Error (MSE)\n",
    "- Mean Absolute Error (MAE)\n",
    "\n",
    "Then on to the new experiment! (3 times is good enough I'd say:))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7e0145",
   "metadata": {},
   "source": [
    "print(f\"Model Name: baklava\")\n",
    "print(f\"Model Version: {version}\")\n",
    "print(f\"Source model format: onnx\")\n",
    "print(f\"Source model format version: 1\")\n",
    "print(f\"Model Path: models/{version}/baklava/\")\n",
    "print(f\"Epochs: {epochs}\")\n",
    "print(f\"Batch Size: {batch_size}\")\n",
    "print(f\"Learning Rate: {learning_rate}\")\n",
    "print(f\"Hidden Layer Units: {hidden_layer_units}\")\n",
    "print(f\"Mean Squared Error: {mse:.4f}\")\n",
    "print(f\"Mean Absolute Error: {mae:.4f}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
