{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51475f0a-7266-475e-96ed-d617d5380221",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -q optimum[onnxruntime]==1.23.3 transformers==4.46.3 torch==2.2.2+cu121\n",
    "import time\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668a7498-c501-401b-9dab-8be1aa49528e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## we need some preprocessing functions from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e949b6a8-2c04-418b-bd5c-dc0a082b7b67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from optimum.onnxruntime import ORTModelForImageClassification\n",
    "from transformers import AutoFeatureExtractor\n",
    "\n",
    "# Load and export the model to ONNX\n",
    "model = ORTModelForImageClassification.from_pretrained(\"rh-ai-bu/wildfire01\", export=True)\n",
    "\n",
    "# Load the feature extractor\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(\"rh-ai-bu/wildfire01\")\n",
    "\n",
    "# Create an ONNX-based pipeline\n",
    "from optimum.pipelines import pipeline\n",
    "onnx_pipeline = pipeline(\"image-classification\", model=model, feature_extractor=feature_extractor, accelerator=\"ort\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850b57f0-a88e-4e0d-bdce-ed56c6142be1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "onnx_pipeline.save_pretrained(\"onnx_pipeline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98c6bea-b3dc-48ac-bc2d-a3d9cd8494b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# image = \"pic2.jpg\"\n",
    "# Import the function from the script\n",
    "from get_random_image import get_random_image\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8bfedf-8263-4b7a-ac82-14f30f6e0b87",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## figure out the precise URL we need to hit:\n",
    "\n",
    "import subprocess\n",
    "\n",
    "def run_command(command):\n",
    "    \"\"\"Run an OS command and return its output, handling errors.\"\"\"\n",
    "    try:\n",
    "        result = subprocess.check_output(command, shell=True, text=True).strip()\n",
    "        return result\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error occurred while running the command '{command}': {e}\")\n",
    "        return None\n",
    "\n",
    "# Commands to run\n",
    "get_IS_name = \"oc get inferenceservice -o jsonpath='{.items[0].metadata.name}'\"\n",
    "get_IS_URL = \"oc get inferenceservice -o jsonpath='{.items[0].status.url}'\"\n",
    "\n",
    "# Retrieve results\n",
    "deployed_model_name = run_command(get_IS_name)\n",
    "infer_endpoint = run_command(get_IS_URL)\n",
    "\n",
    "# Print the results\n",
    "if deployed_model_name:\n",
    "    print(\"Inference Service Name:\", deployed_model_name)\n",
    "else:\n",
    "    print(\"Failed to retrieve Inference Service Name.\")\n",
    "\n",
    "if infer_endpoint:\n",
    "    print(\"Inference Service URL:\", infer_endpoint)\n",
    "else:\n",
    "    print(\"Failed to retrieve Inference Service URL.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94e54b2-c377-4f54-93bb-f5bf04e17aec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "infer_url = f\"{infer_endpoint}/v2/models/{deployed_model_name}/infer\"\n",
    "print(infer_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a5a067-f46c-4a2f-a45f-765b03e1b13c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def rest_request(data,shape):\n",
    "    json_data = {\n",
    "        \"inputs\": [\n",
    "            {\n",
    "                \"name\": \"pixel_values\",\n",
    "                \"shape\": shape,\n",
    "                \"datatype\": \"FP32\",\n",
    "                \"data\": data\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    start_time = time.time()  # Record the start time\n",
    "    response = requests.post(infer_url, json=json_data, verify=True)\n",
    "    end_time = time.time()  # Record the end time\n",
    "\n",
    "    # Calculate and print the response time\n",
    "    response_time = end_time - start_time\n",
    "    print(f\"Response time: {response_time:.6f} seconds\")\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65656d8-af3d-4b7e-98e1-cca1f633bcbe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "image = get_random_image()\n",
    "print(image)\n",
    "img_preprocessed = onnx_pipeline.preprocess(image)\n",
    "img_preprocessed[\"pixel_values\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2cc5230-c109-4ff3-96e7-09efc4b5daac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print(img_preprocessed)\n",
    "\n",
    "prediction = rest_request(img_preprocessed[\"pixel_values\"].tolist(),img_preprocessed[\"pixel_values\"].shape)\n",
    "# prediction\n",
    "\n",
    "\n",
    "# prediction = rest_request(image_preprocessed[\"pixel_values\"].tolist())\n",
    "\n",
    "\n",
    "\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7eb873e-e587-4a73-83eb-9d9df41680a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c8ddf8-4dac-4fc9-b731-db1791248d0f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# prediction.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50441442-10ea-4bc3-94f4-e96010d6763e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_predict(max_iter):\n",
    "    for i in range(1, max_iter + 1):\n",
    "        print(\"Image \"+str(i))\n",
    "        image = get_random_image()\n",
    "        img_preprocessed = onnx_pipeline.preprocess(image)\n",
    "\n",
    "        prediction = rest_request(img_preprocessed[\"pixel_values\"].tolist(),img_preprocessed[\"pixel_values\"].shape)\n",
    "\n",
    "\n",
    "get_predict(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52454701-8a52-493a-b38d-f72809114081",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "def get_predict(max_iter):\n",
    "    start_time = time.time()\n",
    "    for i in range(1, max_iter + 1):\n",
    "        print(\"Image \" + str(i))\n",
    "        image = get_random_image()\n",
    "        img_preprocessed = onnx_pipeline.preprocess(image)\n",
    "\n",
    "        prediction = rest_request(img_preprocessed[\"pixel_values\"].tolist(), img_preprocessed[\"pixel_values\"].shape)\n",
    "\n",
    "        # Check if 60 seconds have passed\n",
    "        if time.time() - start_time > 60:\n",
    "            break\n",
    "\n",
    "# Call the function with a large number, it will stop after 60 seconds\n",
    "get_predict(1000000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ad1f69-6242-454e-ae7d-5bfafa69a243",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import requests\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# Load the ONNX model pipeline (assuming this is already set up as in your original code)\n",
    "from optimum.onnxruntime import ORTModelForImageClassification\n",
    "from transformers import AutoFeatureExtractor\n",
    "from optimum.pipelines import pipeline\n",
    "from get_random_image import get_random_image\n",
    "\n",
    "# Load and export the model to ONNX\n",
    "model = ORTModelForImageClassification.from_pretrained(\"rh-ai-bu/wildfire01\", export=True)\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(\"rh-ai-bu/wildfire01\")\n",
    "onnx_pipeline = pipeline(\"image-classification\", model=model, feature_extractor=feature_extractor, accelerator=\"ort\")\n",
    "onnx_pipeline.save_pretrained(\"onnx_pipeline\")\n",
    "\n",
    "# Set up the endpoint\n",
    "deployed_model_name = \"wildfire01\"\n",
    "infer_endpoint = \"https://wildfire01-user1-ai-mazing.apps.prod.rhoai.rh-aiservices-bu.com\"\n",
    "infer_url = f\"{infer_endpoint}/v2/models/{deployed_model_name}/infer\"\n",
    "\n",
    "# Function to send a single REST request\n",
    "def rest_request(data):\n",
    "    json_data = {\n",
    "        \"inputs\": [\n",
    "            {\n",
    "                \"name\": \"pixel_values\",\n",
    "                \"shape\": data.shape,\n",
    "                \"datatype\": \"FP32\",\n",
    "                \"data\": data.tolist()\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    start_time = time.time()\n",
    "    response = requests.post(infer_url, json=json_data, verify=True)\n",
    "    end_time = time.time()\n",
    "\n",
    "    response_time = end_time - start_time\n",
    "    print(f\"Response time: {response_time:.6f} seconds\")\n",
    "\n",
    "    return response\n",
    "\n",
    "# Preprocess the image\n",
    "image = get_random_image()\n",
    "print(f\"Selected image: {image}\")\n",
    "image_preprocessed = onnx_pipeline.preprocess(image)\n",
    "pixel_values = image_preprocessed[\"pixel_values\"]\n",
    "\n",
    "# Function to handle a single prediction request\n",
    "def process_request():\n",
    "    response = rest_request(pixel_values)\n",
    "    if response.status_code == 200:\n",
    "        prediction_data = response.json()\n",
    "        logits = prediction_data['outputs'][0]['data']\n",
    "        scores = softmax(logits)\n",
    "        return scores\n",
    "    else:\n",
    "        print(f\"Request failed with status code: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Softmax function\n",
    "def softmax(logits):\n",
    "    exp_logits = np.exp(logits - np.max(logits))\n",
    "    return exp_logits / np.sum(exp_logits)\n",
    "\n",
    "# Send 100 simultaneous requests using ThreadPoolExecutor\n",
    "def send_simultaneous_requests():\n",
    "    results = []\n",
    "    with ThreadPoolExecutor(max_workers=100) as executor:\n",
    "        futures = [executor.submit(process_request) for _ in range(100)]\n",
    "        for future in as_completed(futures):\n",
    "            try:\n",
    "                result = future.result()\n",
    "                if result is not None:  # Correctly check for None instead of relying on NumPy truth value\n",
    "                    results.append(result)\n",
    "            except Exception as e:\n",
    "                print(f\"Error during request processing: {e}\")\n",
    "    return results\n",
    "\n",
    "# Execute the simultaneous requests\n",
    "if __name__ == \"__main__\":\n",
    "    all_results = send_simultaneous_requests()\n",
    "    # print(\"All results:\", all_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd945d61-90c0-424e-960f-1899b47cdce5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
