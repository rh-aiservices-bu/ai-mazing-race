{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57e87e83-b457-48a9-8293-4f6ca4de92b4",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Deployin the model in RHOAI single-model serving (via code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44a9ba2-3529-4b65-91b7-4a4629d1a9fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# if you need to delete things, uncomment these 2 lines:\n",
    "# oc delete inferenceservices wildfire01\n",
    "# oc delete servingruntime wildfire01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73638047",
   "metadata": {},
   "source": [
    "## Create the \"ServingRuntime\" object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae50a4b9-b1f1-459a-8ef6-ecf818a1253a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "cat << EOF | oc apply -f-\n",
    "---\n",
    "apiVersion: serving.kserve.io/v1alpha1\n",
    "kind: ServingRuntime\n",
    "metadata:\n",
    "  annotations:\n",
    "    opendatahub.io/accelerator-name: ''\n",
    "    opendatahub.io/apiProtocol: REST\n",
    "    opendatahub.io/recommended-accelerators: '[\"nvidia.com/gpu\"]'\n",
    "    opendatahub.io/template-display-name: OpenVINO Model Server\n",
    "    opendatahub.io/template-name: kserve-ovms\n",
    "    openshift.io/display-name: wildfire01\n",
    "  name: wildfire01\n",
    "  labels:\n",
    "    opendatahub.io/dashboard: 'true'\n",
    "spec:\n",
    "  annotations:\n",
    "    prometheus.io/path: /metrics\n",
    "    prometheus.io/port: '8888'\n",
    "  containers:\n",
    "    - args:\n",
    "        - '--model_name={{.Name}}'\n",
    "        - '--port=8001'\n",
    "        - '--rest_port=8888'\n",
    "        - '--model_path=/mnt/models'\n",
    "        - '--file_system_poll_wait_seconds=0'\n",
    "        - '--grpc_bind_address=0.0.0.0'\n",
    "        - '--rest_bind_address=0.0.0.0'\n",
    "        - '--target_device=AUTO'\n",
    "        - '--metrics_enable'\n",
    "      image: 'quay.io/modh/openvino_model_server@sha256:f1140e9d987580d1aab1ccc62519b48b1d2673308b2db496e9e505e3be788d9f'\n",
    "      name: kserve-container\n",
    "      ports:\n",
    "        - containerPort: 8888\n",
    "          protocol: TCP\n",
    "      volumeMounts:\n",
    "        - mountPath: /dev/shm\n",
    "          name: shm\n",
    "  multiModel: false\n",
    "  protocolVersions:\n",
    "    - v2\n",
    "    - grpc-v2\n",
    "  supportedModelFormats:\n",
    "    - autoSelect: true\n",
    "      name: openvino_ir\n",
    "      version: opset13\n",
    "    - name: onnx\n",
    "      version: '1'\n",
    "    - autoSelect: true\n",
    "      name: tensorflow\n",
    "      version: '1'\n",
    "    - autoSelect: true\n",
    "      name: tensorflow\n",
    "      version: '2'\n",
    "    - autoSelect: true\n",
    "      name: paddle\n",
    "      version: '2'\n",
    "    - autoSelect: true\n",
    "      name: pytorch\n",
    "      version: '2'\n",
    "  volumes:\n",
    "    - emptyDir:\n",
    "        medium: Memory\n",
    "        sizeLimit: 2Gi\n",
    "      name: shm\n",
    "\n",
    "EOF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eaf842b",
   "metadata": {},
   "source": [
    "## Create the \"InferenceService\" Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f18093f-4633-469c-a327-c57799b1acfa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "cat << EOF | oc apply -f-\n",
    "---\n",
    "apiVersion: serving.kserve.io/v1beta1\n",
    "kind: InferenceService\n",
    "metadata:\n",
    "  annotations:\n",
    "    openshift.io/display-name: wildfire01\n",
    "    serving.knative.openshift.io/enablePassthrough: 'true'\n",
    "    sidecar.istio.io/inject: 'true'\n",
    "    sidecar.istio.io/rewriteAppHTTPProbers: 'true'\n",
    "  name: wildfire01\n",
    "  finalizers:\n",
    "    - inferenceservice.finalizers\n",
    "  labels:\n",
    "    # networking.knative.dev/visibility: cluster-local\n",
    "    opendatahub.io/dashboard: 'true'\n",
    "spec:\n",
    "  predictor:\n",
    "    maxReplicas: 1\n",
    "    minReplicas: 1\n",
    "    model:\n",
    "      modelFormat:\n",
    "        name: onnx\n",
    "        version: '1'\n",
    "      name: ''\n",
    "      resources:\n",
    "        limits:\n",
    "          cpu: '1'\n",
    "          memory: 1100Mi\n",
    "        requests:\n",
    "          cpu: 300m\n",
    "          memory: 10Mi\n",
    "      runtime: wildfire01\n",
    "      storage:\n",
    "        key: aws-connection-minio-wildfire\n",
    "        path: wildfire_onnx/\n",
    "EOF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235b2d6c",
   "metadata": {},
   "source": [
    "## Confirm Objects have been created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e5ef74-398c-4374-92df-3a23aa717855",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "oc get ServingRuntime,inferenceservice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e093163f",
   "metadata": {},
   "source": [
    "## Wait for pod to become ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee130e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "oc get pods -l service.istio.io/canonical-name=wildfire01-predictor\n",
    "\n",
    "while true; do\n",
    "  POD_STATUS=$(oc get pods -l service.istio.io/canonical-name=wildfire01-predictor -o json | jq '.items[].status.phase' | grep -c \"Running\")\n",
    "  if [ $POD_STATUS -eq $(oc get pods -l service.istio.io/canonical-name=wildfire01-predictor -o json | jq '.items | length') ]; then\n",
    "    echo \"All pods are running\"\n",
    "    break\n",
    "  else\n",
    "    echo \"Not all pods are running. Status: $(oc get pods -l service.istio.io/canonical-name=wildfire01-predictor -o json | jq '.items[].status.phase')\"\n",
    "    sleep 2\n",
    "  fi\n",
    "done\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
